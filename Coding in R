# KNN
library(dplyr)
library(class)


# Train x -> Independence variables

# Train X scale
train_x<- TrainData%>%
dplyr::select(-Y) %>% 
scale()

#Mean train x
mean_train_x<-attr(train_x,"scaled:center")

# Std Dev train x
std_train_x<-attr(train_x,"scaled:scale")

#Train Y -> Dependence variable
#Train Y
train_y<-pull(TrainData, Y)

# Test X 
test_x<- scale(testdata,center = mean_train_x, scale = std_train_x)

# Prediction
yhat_k1 <- knn(train_x, test_x, train_y, k=n)
print(yhat_k)

# Calculate Misclassification
mis_k1 <- mean(test_y!=yhat_k1)
print(mis)

# Calculate Probabilities
yhat_k <- knn(train_x, test_x, train_y, k=n, prob = TRUE)

# Create cross table to calculate Sensitivity & Specificity
table(test_y!=yhat_k1)

# Split the data into 2 parts (Test data & Train data)
ind<-ifelse(runif(n)<0.7,"Training","Test") #n = row number
new_train_x<-train_x[ind=="Training",]
new_test_x<-train_x[ind=="Test",]
new_train_y<- train_y[ind=="Training"]
new_test_y<- train_y[ind=="Test"]

# Prediction
yhat_k_split<-knn(new_train_x,new_test_x,new_train_y,k=n)

# Calculate Misclassification
mean(yhat_k_split!=new_test_y)


# Linear and Quadratic Discriminant Analysis
library(mass)
library(tidyverse)
library(ggplot)

# 1. Test data and Train data in 2 dataframes
# Training data
training_data <- readRDS('Training Data.rds')
test_data <- readRDS('Test Data.rds')

# LDA
lda_output <- lda( Y ~ X, data = training_data)
# Predict LDA
yhat_lda <- predict(lda_output, newdata = test_data)
# Prediction
yhat_lda$class
# Calculate Probabilities
yhat_lda$posterior

# QDA
qda_output <- qda( Y ~ X, data = training_data) # X is(are) independence variable(s)
# Predict QDA
yhat_qda <- predict(qda_output, newdata = test_data)
# Calculate Probabilities
yhat_qda$posterior

# 2. Test data and Train data in one dataframe
Data <- readRDS('Training Data.rds')
# Split data
ind<-ifelse(runif(n)<0.7,"Training","Test") # n= row number

# Add Column
Data_with_Sample<-add_column(Data,Sample=ind)

# Training Data
train_data<-Data_with_Sample%>%
filter(Sample=="Training")%>%
select(-Sample) #Can remove Sample variable

# Test Data
test_data<-Data_with_Sample%>%
filter(Sample=="Test")%>%
select(-Sample) #Can remove Sample variable

# LDA
lda_output <- lda( Y ~ X, data = train_data) # X is(are) independence variable(s)
# Prediction
yhat_lda <- predict(lda_output, newdata = test_data)
# Misclassification
mean(yhat_lda$class != test_data$Y) # Y is dependence variable

# QDA
qda_output <- qda( Y ~ X, data = train_data)
# Prediction
yhat_qda <- predict(qda_output, newdata = test_data)
# Misclassification
mean(yhat_qda$class != test_data$Y)
# Probanilities
yhat_qda$posterior
# Cross table
table(yhat_lda$class != test_data$Y)
table(yhat_lda$class != yhat_qda$class)


# Decision Tree
library(rpart)
library(tidyverse)
library(rpart.plot)

# 1. Test data & Training data in 2 different dataframes
# Training data
training_data<-readRDS('TrainingData.rds')
# Test Data
test_data<-readRDS('TestData.rds')

# Tree plot
tree<- rpart( Y ~ X, data = training_data) # only one indenpdent variable
tree<- rpart( Y ~ ., data = training_data) # Except one dependent variable, others are all independent variables

# Predict Tree
yhat<-predict(tree, newdata = test_data, type = 'class')

# Plot decision tree
rpart.plot(tree)

# Tree with Minimum Observation
tree15<-rpart(Y ~ X, data = training_data, control = rpart.control(minbucket = 15))

# Predict tree
yhat15<-predict(tree15, newdata = test_data, type = 'class')
